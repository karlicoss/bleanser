#!/usr/bin/env python3

from argparse import ArgumentParser


p = ArgumentParser()
p.add_argument('--extract', '-e', action='store_true')
p.add_argument('input')
args = p.parse_args()
inp = args.input

# TODO could be useful to assert cleanup in a strict subset?
def cleanup() -> str:
    sections = [
        'saved',
        'comments',
        'upvoted',
        'downvoted',
        'submissions',
    ]
    def d(q):
        return f'del({q})'
    dq = []
    ignore_keys = [
        'icon_img',
        'icon_size',

        'thumbnail_height',
        # TODO FIXME eh? ./jdiff --diff reddit/reddit-20190213191021.json reddit/reddit-20190213204024.json this is interesting, lots of created' changed. and then back??

        'crosspost_parent_list',
        'primary_color',
        'archived',
        'suggested_sort',
        'over_18',
        'over18',
        'allow_videos',
        'comment_score_hide_mins',
        'wiki_enabled',
        'suggested_sort',
        'suggested_comment_sort',
        'header_img',
        'header_size',
        'has_menu_widget',
        'banner_background_color',
        'banner_background_image',
        'banner_img',
        'banner_size',

        'community_icon',
        'no_follow',
        'submission_type',
        'is_crosspostable',

        'link_flair_enabled',
        'link_flair_position',
        'link_flair_css_class',
        'link_flair_template_id',
        'link_flair_text',
        'link_flair_type',
        'link_flair_richtext',

        'post_hint',
        'is_robot_indexable',
        'content_categories',

        'parent_whitelist_status',
        'pwls',
        'whitelist_status',
        'wls',
        'show_media',
        'spoilers_enabled',
        'collapse_deleted_comments',
        'key_color',
        'can_assign_user_flair',
        'emojis_enabled',
        'author_patreon_flair',
        "author_flair_richtext",
        'author_flair_text',
        'author_flair_background_color',
        'author_flair_text_color',
        'author_flair_type',
        'author_flair_css_class',
        'author_flair_template_id',

        "original_content_tag_enabled",
        'emojis_custom_size',

        'gilded',
        'gid_2',
        'gid_3',
        'media_metadata',
        'can_assign_link_flair',
    ]
    # TODO shit. looks like query might be too long for jq...
    # TODO careful about that one!!
    for q in range(0, len(ignore_keys), 10):
        aa = ignore_keys[q: q + 10]
        dq.append(d('.. | ({})'.format(', '.join('.' + a + '?' for a in aa))))
    dq.extend([
        d(f'.{section}[] | (.preview, .body_html, .score, .ups, .description_html, .subreddit_type, .subreddit_subscribers, .selftext_html, .num_comments, .num_crossposts, .thumbnail, .created)') for section in sections
    ])
    dq.append(
        d('.multireddits[] | (.description_html, .created)')
    )
    # del_preview = lambda s: ddel(f'.{s} | .[]')
    # dq.extend(del_preview(s) for s in sections)
    # TODO shit, that's gonna remove lots of subreddits
    # I should also check that result contains reasonable favorites??
    # TODO not sure if it's necessary to sort.. | sort_by(.id) 
    # dq.append('.subreddits | map(del(.subscribers, .videostream_links_count, .description_html))') # ddel('(.subreddits) | .)') #  | del(.videostream_links_count) | del(.description_html)
    dq.extend([
        d('.subreddits[] | (.created, .subscribers, .description, .description_html, .videostream_links_count, .submit_text, .submit_text_html)'),
    ])
    return ' | '.join(dq)

def extract():
    qs = [
        # TODO FIXME this should be assertive on field existence

        # hmm, created changes all the time for some reason starting from 20181124201020
        # https://www.reddit.com/r/redditdev/comments/29991t/whats_the_difference_between_created_and_created/ciiuk24/
        # ok, it's broken

        '.comments     |= map({id, created_utc, body})',
        '.multireddits |= map({id, created_utc, name, subreddits: .subreddits | map_values(.display_name) })',
        '.saved        |= map({id, created_utc, body})',
        '.submissions  |= map({id, created_utc, title, selftext})',
        '.subreddits   |= map({id, created_utc, title, display_name, public_description})',
        '.upvoted      |= map({id, created_utc, title, selftext})',
        '.downvoted    |= map({id, created_utc, title, selftext})',

        # '.comments     |= []',
        # '.multireddits |= []',
        # '.profile      |= {}',
        # '.saved        |= []',
        # '.submissions  |= []',
        # '.subreddits   |= []',
        # '.upvoted      |= []',
    ]
    # TODO remove line breaks?
    return ' | '.join(qs)

# TODO also check mode?? diff if and only if
q = extract if args.extract else cleanup

# 2 styles of normalising:
# first is extracting stuff we expect to see. this is nicer and gives the idea if something actually changed
# second is cleaning up stuff that we don't need

# INPUT="$1"
# jq 'def(.saved.[].preview.images)' "$INPUT"
import subprocess
subprocess.check_call(['jq', q(), inp])
# subreddit subscribers: ignore all

# saved -> * -> preview -> images
# same for upvoted and downvoted?


# TODO nothing should decrease jq . reddit1.json | jq 'map_values(length)' | less 
# jq 'paths'
