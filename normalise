#!/usr/bin/env python3

from argparse import ArgumentParser


p = ArgumentParser()
p.add_argument('--extract', '-e', action='store_true')
p.add_argument('input')
args = p.parse_args()
inp = args.input

def cleanup() -> str:
    sections = [
        'saved',
        'comments',
        'upvoted',
        'downvoted',
        'submissions',
    ]
    def d(q):
        return f'del({q})'
    dq = []
    ignore_keys = [
        'icon_img',
        'crosspost_parent_list',
        'primary_color',
        'archived',
        'suggested_sort',
        'over_18',
        'over18',
        'allow_videos',
        'comment_score_hide_mins',
        'wiki_enabled',
        'suggested_sort',
        'suggested_comment_sort',
        'banner_background_image',
        'header_img',
        'header_size',
        'community_icon',
    ]
    dq.append(d('.. | ({})'.format(', '.join('.' + a + '?' for a in ignore_keys))))
    dq.extend([
        d(f'.{section}[] | (.preview, .body_html, .score, .ups, .description_html, .subreddit_type, .subreddit_subscribers, .selftext_html, .num_comments)') for section in sections
    ])
    dq.append(
        d('.multireddits[] | .description_html')
    )
    # del_preview = lambda s: ddel(f'.{s} | .[]')
    # dq.extend(del_preview(s) for s in sections)
    # TODO shit, that's gonna remove lots of subreddits
    # I should also check that result contains reasonable favorites??
    # TODO not sure if it's necessary to sort.. | sort_by(.id) 
    # dq.append('.subreddits | map(del(.subscribers, .videostream_links_count, .description_html))') # ddel('(.subreddits) | .)') #  | del(.videostream_links_count) | del(.description_html)
    dq.extend([
        d('.subreddits[] | (.subscribers, .description, .description_html, .videostream_links_count)'),
    ])
    return ' | '.join(dq)

def extract():
    qs = [
        # TODO FIXME this should be assertive on field existence
        '.comments     |= map({id, body})',
        '.multireddits |= map({name, subreddits: .subreddits | map_values(.display_name) })',
        '.saved        |= map({id, body})',
        '.submissions  |= map({id, title, selftext})',
        '.subreddits   |= map({id, display_name})',
        '.upvoted      |= map({id, title, selftext})',
        '.downvoted    |= map({id, title, selftext})',

        # '.comments     |= []',
        # '.multireddits |= []',
        # '.profile      |= {}',
        # '.saved        |= []',
        # '.submissions  |= []',
        # '.subreddits   |= []',
        # '.upvoted      |= []',
    ]
    # TODO remove line breaks?
    return ' | '.join(qs)

# TODO also check mode?? diff if and only if
q = extract if args.extract else cleanup

# 2 styles of normalising:
# first is extracting stuff we expect to see. this is nicer and gives the idea if something actually changed
# second is cleaning up stuff that we don't need

# INPUT="$1"
# jq 'def(.saved.[].preview.images)' "$INPUT"
import subprocess
subprocess.check_call(['jq', q(), inp])
# subreddit subscribers: ignore all

# saved -> * -> preview -> images
# same for upvoted and downvoted?


# TODO nothing should decrease jq . reddit1.json | jq 'map_values(length)' | less 
# jq 'paths'
